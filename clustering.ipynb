{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: __main__.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: __main__.py [options]\n",
      "\n",
      "__main__.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# parse commandine arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "(opts, args) = op.parse_args()\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load categories\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "18846 documents\n",
      "20 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "# dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
    "print(\"%d documents\" %len(dataset.data))\n",
    "print(\"%d categories\" %len(dataset.target_names))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"use_hashing\": False,\n",
    "    \"use_idf\": True,\n",
    "    \"n_features\": 1000,\n",
    "    \"n_components\": 10,\n",
    "    \"verbose\": True,\n",
    "    \"minibatch\": True,\n",
    "    \"max_features\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using sparse vectorizer\n",
      "done in 7.819377s\n",
      "n_samples: 18846, n_features: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from the training dataset using sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts[\"use_hashing\"]:\n",
    "    if opts[\"use_idf\"]:\n",
    "        hasher = HashingVectorizer(n_features=opts[\"n_features\"],\n",
    "                                  stop_words='english',\n",
    "                                  non_negative=True,\n",
    "                                  norm=None,\n",
    "                                  binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts[\"n_features\"],\n",
    "                                      stop_words='english',\n",
    "                                      non_negative=False,\n",
    "                                      norm='l2',\n",
    "                                      binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                                 max_features=opts[\"max_features\"],\n",
    "                                 min_df=2, \n",
    "                                 stop_words='english', \n",
    "                                 use_idf=opts[\"use_idf\"])\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "    \n",
    "print(\"done in %fs\"%(time()-t0))\n",
    "print(\"n_samples: %d, n_features: %d\" %X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.588839s\n",
      "Explained variance of the SVD step: 2%\n"
     ]
    }
   ],
   "source": [
    "if opts[\"n_components\"]:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    svd = TruncatedSVD(opts[\"n_components\"])\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    \n",
    "    X = lsa.fit_transform(X)\n",
    "    \n",
    "    print(\"done in %fs\" %(time()-t0))\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "        init_size=1000, max_iter=100, max_no_improvement=10, n_clusters=20,\n",
      "        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=True)\n",
      "Init 1/1 with method: k-means++\n",
      "Inertia for init 1/1: 154.788355\n",
      "Minibatch iteration 1/1900: mean batch inertia: 0.158466, ewa inertia: 0.158466 \n",
      "Minibatch iteration 2/1900: mean batch inertia: 0.153343, ewa inertia: 0.157923 \n",
      "Minibatch iteration 3/1900: mean batch inertia: 0.154056, ewa inertia: 0.157512 \n",
      "Minibatch iteration 4/1900: mean batch inertia: 0.151317, ewa inertia: 0.156855 \n",
      "Minibatch iteration 5/1900: mean batch inertia: 0.158126, ewa inertia: 0.156990 \n",
      "Minibatch iteration 6/1900: mean batch inertia: 0.158290, ewa inertia: 0.157128 \n",
      "Minibatch iteration 7/1900: mean batch inertia: 0.154333, ewa inertia: 0.156831 \n",
      "Minibatch iteration 8/1900: mean batch inertia: 0.152472, ewa inertia: 0.156368 \n",
      "Minibatch iteration 9/1900: mean batch inertia: 0.152726, ewa inertia: 0.155982 \n",
      "Minibatch iteration 10/1900: mean batch inertia: 0.151664, ewa inertia: 0.155524 \n",
      "Minibatch iteration 11/1900: mean batch inertia: 0.154156, ewa inertia: 0.155379 \n",
      "Minibatch iteration 12/1900: mean batch inertia: 0.155209, ewa inertia: 0.155361 \n",
      "Minibatch iteration 13/1900: mean batch inertia: 0.153575, ewa inertia: 0.155171 \n",
      "Minibatch iteration 14/1900: mean batch inertia: 0.155988, ewa inertia: 0.155258 \n",
      "Minibatch iteration 15/1900: mean batch inertia: 0.152049, ewa inertia: 0.154917 \n",
      "Minibatch iteration 16/1900: mean batch inertia: 0.153094, ewa inertia: 0.154724 \n",
      "Minibatch iteration 17/1900: mean batch inertia: 0.152681, ewa inertia: 0.154507 \n",
      "Minibatch iteration 18/1900: mean batch inertia: 0.154725, ewa inertia: 0.154530 \n",
      "Minibatch iteration 19/1900: mean batch inertia: 0.153605, ewa inertia: 0.154432 \n",
      "Minibatch iteration 20/1900: mean batch inertia: 0.152620, ewa inertia: 0.154240 \n",
      "Minibatch iteration 21/1900: mean batch inertia: 0.154542, ewa inertia: 0.154272 \n",
      "Minibatch iteration 22/1900: mean batch inertia: 0.155329, ewa inertia: 0.154384 \n",
      "Minibatch iteration 23/1900: mean batch inertia: 0.149953, ewa inertia: 0.153914 \n",
      "Minibatch iteration 24/1900: mean batch inertia: 0.155180, ewa inertia: 0.154048 \n",
      "Minibatch iteration 25/1900: mean batch inertia: 0.155121, ewa inertia: 0.154162 \n",
      "Minibatch iteration 26/1900: mean batch inertia: 0.149959, ewa inertia: 0.153716 \n",
      "Minibatch iteration 27/1900: mean batch inertia: 0.152708, ewa inertia: 0.153609 \n",
      "Minibatch iteration 28/1900: mean batch inertia: 0.150730, ewa inertia: 0.153303 \n",
      "Minibatch iteration 29/1900: mean batch inertia: 0.155581, ewa inertia: 0.153545 \n",
      "Minibatch iteration 30/1900: mean batch inertia: 0.151800, ewa inertia: 0.153360 \n",
      "Minibatch iteration 31/1900: mean batch inertia: 0.151764, ewa inertia: 0.153191 \n",
      "Minibatch iteration 32/1900: mean batch inertia: 0.156522, ewa inertia: 0.153544 \n",
      "Minibatch iteration 33/1900: mean batch inertia: 0.155753, ewa inertia: 0.153778 \n",
      "Minibatch iteration 34/1900: mean batch inertia: 0.150697, ewa inertia: 0.153451 \n",
      "Minibatch iteration 35/1900: mean batch inertia: 0.157549, ewa inertia: 0.153886 \n",
      "Minibatch iteration 36/1900: mean batch inertia: 0.149614, ewa inertia: 0.153433 \n",
      "Minibatch iteration 37/1900: mean batch inertia: 0.152703, ewa inertia: 0.153355 \n",
      "Minibatch iteration 38/1900: mean batch inertia: 0.154830, ewa inertia: 0.153512 \n",
      "Minibatch iteration 39/1900: mean batch inertia: 0.154764, ewa inertia: 0.153645 \n",
      "Minibatch iteration 40/1900: mean batch inertia: 0.151977, ewa inertia: 0.153468 \n",
      "Minibatch iteration 41/1900: mean batch inertia: 0.145080, ewa inertia: 0.152578 \n",
      "Minibatch iteration 42/1900: mean batch inertia: 0.154574, ewa inertia: 0.152789 \n",
      "Minibatch iteration 43/1900: mean batch inertia: 0.151293, ewa inertia: 0.152631 \n",
      "Minibatch iteration 44/1900: mean batch inertia: 0.152672, ewa inertia: 0.152635 \n",
      "Minibatch iteration 45/1900: mean batch inertia: 0.155024, ewa inertia: 0.152888 \n",
      "Minibatch iteration 46/1900: mean batch inertia: 0.155734, ewa inertia: 0.153190 \n",
      "Minibatch iteration 47/1900: mean batch inertia: 0.150699, ewa inertia: 0.152926 \n",
      "Minibatch iteration 48/1900: mean batch inertia: 0.148749, ewa inertia: 0.152483 \n",
      "Minibatch iteration 49/1900: mean batch inertia: 0.156494, ewa inertia: 0.152908 \n",
      "Minibatch iteration 50/1900: mean batch inertia: 0.158266, ewa inertia: 0.153477 \n",
      "Minibatch iteration 51/1900: mean batch inertia: 0.151423, ewa inertia: 0.153259 \n",
      "Minibatch iteration 52/1900: mean batch inertia: 0.155312, ewa inertia: 0.153477 \n",
      "Minibatch iteration 53/1900: mean batch inertia: 0.154432, ewa inertia: 0.153578 \n",
      "Minibatch iteration 54/1900: mean batch inertia: 0.147935, ewa inertia: 0.152979 \n",
      "Minibatch iteration 55/1900: mean batch inertia: 0.154971, ewa inertia: 0.153191 \n",
      "Minibatch iteration 56/1900: mean batch inertia: 0.153129, ewa inertia: 0.153184 \n",
      "Minibatch iteration 57/1900: mean batch inertia: 0.152997, ewa inertia: 0.153164 \n",
      "Minibatch iteration 58/1900: mean batch inertia: 0.153579, ewa inertia: 0.153208 \n",
      "Converged (lack of improvement in inertia) at iteration 58/1900\n",
      "Computing label assignment and total inertia\n",
      "done in 0.165s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if opts[\"minibatch\"]:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, \n",
    "                        init='k-means++',\n",
    "                        n_init=1,\n",
    "                        init_size=1000,\n",
    "                        batch_size=1000,\n",
    "                        verbose=opts[\"verbose\"])\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k,\n",
    "               init='k-means++',\n",
    "               max_iter=100,\n",
    "               n_init=1,\n",
    "               verbose=opts[\"verbose\"])\n",
    "    \n",
    "print (\"Clustering sparse data with %s\"%km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" %(time()-t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.397\n",
      "Completeness: 0.402\n",
      "V-measure: 0.399\n",
      "Adjusted Rand-Index: 0.243\n",
      "Silhouette Coefficient: 0.210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: com windows dos file netcom ibm window posting use article\n",
      "Cluster 1: com article people university don just posting like host cs\n",
      "Cluster 2: people don com just think government like article know israel\n",
      "Cluster 3: com ca university article posting host nntp cs game like\n",
      "Cluster 4: drive scsi university posting host nntp ohio sale state ide\n",
      "Cluster 5: com article people stratus netcom government don just hp like\n",
      "Cluster 6: god jesus people bible christian believe don christ christians say\n",
      "Cluster 7: windows dos university use file thanks know card like drive\n",
      "Cluster 8: key clipper chip encryption com government netcom keys escrow access\n",
      "Cluster 9: com article netcom hp sun stratus posting ibm nntp host\n",
      "Cluster 10: game ca team games hockey year university don players cs\n",
      "Cluster 11: ohio state cleveland cwru magnus university acs host nntp posting\n",
      "Cluster 12: people israel armenian israeli jews turkish armenians government don just\n",
      "Cluster 13: windows dos file window program files thanks ms use help\n",
      "Cluster 14: nasa space gov access digex henry pat net shuttle jpl\n",
      "Cluster 15: university uk posting host nntp ac ca cs ohio thanks\n",
      "Cluster 16: uk ac university cs ca posting host nntp article mail\n",
      "Cluster 17: drive scsi ide card disk drives controller hard mac bus\n",
      "Cluster 18: nasa space gov access digex henry article ca cs like\n",
      "Cluster 19: com article netcom posting don ca nntp host like hp\n"
     ]
    }
   ],
   "source": [
    "if not opts[\"use_hashing\"]:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts[\"n_components\"]:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'use_hashing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-faa9d9dd81f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_hashing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'use_hashing'"
     ]
    }
   ],
   "source": [
    "opts[\"use_hashing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
